{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"./Dataset/EURUSD/EURUSD_M30_with_features+label3.csv\"\n",
    "# FILE_PATH = \"./Dataset/EURUSD/EURUSD_M30_features+label_v.2.1.csv\"\n",
    "data = pd.read_csv(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and labels\n",
    "features = ['SMA100', 'RSI14', 'Close']\n",
    "# features = ['Open', 'High', 'Low', 'Close', 'Volume', 'SMA200', 'SMA100', 'SMA50', 'SMA13', 'RSI14']\n",
    "# features = ['Close', 'SMA200', 'SMA50', 'RSI14']\n",
    "labels = 'signal'\n",
    "X = data[features].values\n",
    "y = data[labels].values\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert labels to integers\n",
    "num_classes = len(set(y))\n",
    "y = torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stephen\\AppData\\Local\\Temp\\ipykernel_8504\\663111827.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
      "C:\\Users\\Stephen\\AppData\\Local\\Temp\\ipykernel_8504\\663111827.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 15\n",
    "num_layers = 2\n",
    "num_heads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, labels, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        X_seq.append(data[i:i + seq_length])\n",
    "        y_seq.append(labels[i + seq_length - 1])  # Directly append the label without .item()\n",
    "    return torch.stack(X_seq), torch.tensor(y_seq, dtype=torch.long)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_tensor, y_train_tensor, seq_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_tensor, y_val_tensor, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding class for the Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForexTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, seq_length, num_heads, num_layers, output_dim):\n",
    "        super(ForexTransformer, self).__init__()\n",
    "        self.embed_dim = 128  # Increased embedding dimension\n",
    "        self.embedding = nn.Linear(input_dim, self.embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(self.embed_dim, max_len=seq_length)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.embed_dim, nhead=num_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(0.4)  # Increased dropout rate\n",
    "        self.fc = nn.Linear(self.embed_dim * seq_length, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.dropout(x)  # Apply dropout after transformer\n",
    "        x = x.flatten(start_dim=1)  # Flatten for the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([159906, 15, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "model = ForexTransformer(input_dim=X_train_seq.shape[2], seq_length=seq_length, num_heads=num_heads, num_layers=num_layers, output_dim=num_classes).to('cuda')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = \"D:/Programing/AI Trader/Model/transformerModel+dropout_Labeled_layers-heads 4-8_Val-Loss 0.8701, Val-Accuracy 0.5773_at 20241216-054736.model\"\n",
    "# model.load_state_dict(torch.load(MODEL_PATH))\n",
    "# model.eval()\n",
    "# print()\n",
    "# print(f\"Model: {MODEL_PATH.split('/')[-1]} is loaded.\")\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   1%|          | 1/100 [00:26<43:21, 26.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Loss: 1.1270 ; Val-Loss: 1.0568 ; Val-Accuracy: 0.4462 ; @ 20241222-015224\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   2%|▏         | 2/100 [00:52<42:44, 26.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Loss: 1.0608 ; Val-Loss: 1.0474 ; Val-Accuracy: 0.4559 ; @ 20241222-015250\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   3%|▎         | 3/100 [01:18<42:16, 26.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Loss: 1.0556 ; Val-Loss: 1.0487 ; Val-Accuracy: 0.4511 ; @ 20241222-015316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   4%|▍         | 4/100 [01:44<41:54, 26.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Loss: 1.0475 ; Val-Loss: 1.0379 ; Val-Accuracy: 0.4518 ; @ 20241222-015343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   5%|▌         | 5/100 [02:11<41:34, 26.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Loss: 1.0316 ; Val-Loss: 1.0154 ; Val-Accuracy: 0.4905 ; @ 20241222-015409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def train_model(model, X_train_seq, y_train_seq, X_val_seq, y_val_seq, epochs=100, batch_size=1024):\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train_seq, y_train_seq)\n",
    "    val_dataset = torch.utils.data.TensorDataset(X_val_seq, y_val_seq)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Epochs\"):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to('cuda'), batch_y.to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to('cuda'), batch_y.to('cuda')\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "        \n",
    "        \n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        \n",
    "        print(f\"Train-Loss: {train_loss:.4f} ; Val-Loss: {val_loss:.4f} ; Val-Accuracy: {val_accuracy:.4f} ; @ {timestamp}\\n\")\n",
    "        \n",
    "        MODEL_SAVEPATH = f\"./Model/transformerModelv.3.0_layers-heads {num_layers}-{num_heads}_Val-Loss {val_loss:.4f}, Val-Accuracy {val_accuracy:.4f}_at {timestamp}.model\"\n",
    "        \n",
    "        torch.save(model.state_dict(), MODEL_SAVEPATH)\n",
    "\n",
    "train_model(model, X_train_seq, y_train_seq, X_val_seq, y_val_seq)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
