{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f90c2dc-fa31-430c-a88a-4b30882b9443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# تنظیم دستگاه (GPU اگر در دسترس باشد، در غیر این صورت CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# مسیر داده‌ها\n",
    "MELBOURNE_DATASET = \"D:/M.A/T2/Deep Learning/Assignments/HM2 - Es.Kiani - 40311614/RBF/daily-minimum-temperatures-in-melbourne.csv\"\n",
    "\n",
    "# توابع فعال‌سازی\n",
    "def sigmoid(x):\n",
    "    return torch.sigmoid(x)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def rbf(x, center, sigma=1.0):\n",
    "    return torch.exp(-torch.sum((x - center) ** 2, dim=-1) / (2 * sigma ** 2))\n",
    "\n",
    "def rbf_derivative(x, center, sigma=1.0):\n",
    "    rbf_val = rbf(x, center, sigma)\n",
    "    return -rbf_val * (x - center) / (sigma ** 2)\n",
    "\n",
    "def leaky_relu(x, alpha=0.1):\n",
    "    return torch.where(x > 0, x, alpha * x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.1):\n",
    "    return torch.where(x > 0, torch.ones_like(x), alpha * torch.ones_like(x))\n",
    "\n",
    "# آماده‌سازی سری زمانی\n",
    "def create_time_series(data, num_inputs, steps_ahead=3):\n",
    "    X = torch.zeros((data.shape[0] - num_inputs - steps_ahead + 1, num_inputs))\n",
    "    Y = torch.zeros((data.shape[0] - num_inputs - steps_ahead + 1))\n",
    "    for i in range(X.shape[0]):\n",
    "        X[i] = data[i:i + num_inputs]\n",
    "        Y[i] = data[i + num_inputs + steps_ahead - 1]\n",
    "    return X, Y\n",
    "\n",
    "# بارگذاری و نرمال‌سازی داده‌ها\n",
    "def load_and_normalize_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    temperatures = torch.tensor(data['Daily minimum temperatures in Melbourne, Australia, 1981-1990'].to_numpy(), dtype=torch.float32)\n",
    "    min_val, max_val = torch.min(temperatures), torch.max(temperatures)\n",
    "    normalized_data = (temperatures - min_val) / (max_val - min_val)\n",
    "    return normalized_data, min_val, max_val\n",
    "\n",
    "# تقسیم داده‌ها به train/test/val\n",
    "def split_data(X, Y, train_ratio=0.7, val_ratio=0.1):\n",
    "    num_data = X.shape[0]\n",
    "    num_train = int(num_data * train_ratio)\n",
    "    num_val = int(num_data * val_ratio)\n",
    "    indices = torch.randperm(num_data)\n",
    "    X, Y = X[indices], Y[indices]\n",
    "    X_train, Y_train = X[:num_train], Y[:num_train]\n",
    "    X_val, Y_val = X[num_train:num_train + num_val], Y[num_train:num_train + num_val]\n",
    "    X_test, Y_test = X[num_train + num_val:], Y[num_train + num_val:]\n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test\n",
    "\n",
    "# هایپرپارامترها\n",
    "num_inputs = 30\n",
    "n1_rbf = 20\n",
    "n2 = 15\n",
    "n3 = 10\n",
    "n1_perceptron = 5\n",
    "n2_perceptron = 1\n",
    "eta_rbf = 0.25\n",
    "eta_auto = 0.25\n",
    "eta_perceptron = 0.25\n",
    "epochs_auto = 50\n",
    "epochs_perceptron = 50\n",
    "sigma_rbf = 0.1\n",
    "\n",
    "# بارگذاری داده‌ها\n",
    "data, min_val, max_val = load_and_normalize_data(MELBOURNE_DATASET)\n",
    "X, Y = create_time_series(data, num_inputs, steps_ahead=3)\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = split_data(X, Y)\n",
    "\n",
    "# انتقال داده‌ها به GPU\n",
    "X_train, Y_train = X_train.to(device), Y_train.to(device)\n",
    "X_test, Y_test = X_test.to(device), Y_test.to(device)\n",
    "\n",
    "# مقداردهی اولیه\n",
    "centers_enc = X_train[torch.randperm(X_train.shape[0])[:n1_rbf]].clone().detach().to(device)  # (20, 30)\n",
    "w2_e = torch.rand(n2, n1_rbf, device=device) * 2 - 1  # (15, 20)\n",
    "w3_e = torch.rand(n3, n2, device=device) * 2 - 1      # (10, 15)\n",
    "w3_d = torch.rand(n2, n3, device=device) * 2 - 1      # (15, 10)\n",
    "w2_d = torch.rand(num_inputs, n2, device=device) * 2 - 1  # (30, 15)\n",
    "w1_p = torch.rand(n1_perceptron, n3, device=device) * 2 - 1  # (5, 10)\n",
    "w2_p = torch.rand(n2_perceptron, n1_perceptron, device=device) * 2 - 1  # (1, 5)\n",
    "\n",
    "# فعال کردن گرادیان برای وزن‌ها\n",
    "w2_e.requires_grad_(True)\n",
    "w3_e.requires_grad_(True)\n",
    "w3_d.requires_grad_(True)\n",
    "w2_d.requires_grad_(True)\n",
    "w1_p.requires_grad_(True)\n",
    "w2_p.requires_grad_(True)\n",
    "\n",
    "# تعریف توابع کمکی برای پیش‌بینی\n",
    "def compute_encoder_output(X):\n",
    "    rbf_layer = torch.stack([rbf(X, c, sigma_rbf) for c in centers_enc], dim=0).T  # (batch, 20)\n",
    "    net2_e = w2_e @ rbf_layer  # (15, 20) @ (20, batch) → (15, batch)\n",
    "    o2_e = leaky_relu(net2_e)  # (15, batch)\n",
    "    net3_e = w3_e @ o2_e       # (10, 15) @ (15, batch) → (10, batch)\n",
    "    o3_e = leaky_relu(net3_e)  # (10, batch)\n",
    "    return o3_e\n",
    "\n",
    "def compute_perceptron_output(autoencoder_output):\n",
    "    net1_p = w1_p @ autoencoder_output  # (5, 10) @ (10,) → (5,)\n",
    "    o1_p = sigmoid(net1_p)              # (5,)\n",
    "    net2_p = w2_p @ o1_p                # (1, 5) @ (5,) → (1,)\n",
    "    o2_p = sigmoid(net2_p)              # (1,)\n",
    "    return o2_p\n",
    "\n",
    "# آموزش Autoencoder\n",
    "for t in range(epochs_auto):\n",
    "    for i in range(X_train.shape[0]):\n",
    "        X = X_train[i].unsqueeze(0)  # (1, 30)\n",
    "\n",
    "        # Feedforward - Encoder\n",
    "        o1_e = torch.stack([rbf(X, c, sigma_rbf) for c in centers_enc], dim=0)  # (20,)\n",
    "        net2_e = w2_e @ o1_e  # (15, 20) @ (20,) → (15,)\n",
    "        o2_e = leaky_relu(net2_e)  # (15,)\n",
    "        net3_e = w3_e @ o2_e  # (10, 15) @ (15,) → (10,)\n",
    "        o3_e = leaky_relu(net3_e)  # (10,)\n",
    "\n",
    "        # Decoder\n",
    "        net3_d = w3_d @ o3_e  # (15, 10) @ (10,) → (15,)\n",
    "        o3_d = leaky_relu(net3_d)  # (15,)\n",
    "        net2_d = w2_d @ o3_d  # (30, 15) @ (15,) → (30,)\n",
    "        o1_d = sigmoid(net2_d)  # (30,)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss = torch.mean((X.squeeze(0) - o1_d) ** 2)  # (30,) - (30,)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # به‌روزرسانی وزن‌ها\n",
    "            w2_e -= eta_auto * w2_e.grad\n",
    "            w3_e -= eta_auto * w3_e.grad\n",
    "            w3_d -= eta_auto * w3_d.grad\n",
    "            w2_d -= eta_auto * w2_d.grad\n",
    "\n",
    "            # به‌روزرسانی مراکز RBF\n",
    "            for j in range(n1_rbf):\n",
    "                grad_center_enc = rbf_derivative(X, centers_enc[j], sigma_rbf).squeeze(0)  # (30,)\n",
    "                delta = torch.sum(w2_e.grad[:, j])  # Scalar from (15,)\n",
    "                centers_enc[j] += eta_rbf * grad_center_enc * delta  # (30,) * scalar\n",
    "\n",
    "            # پاک کردن گرادیان‌ها\n",
    "            w2_e.grad.zero_()\n",
    "            w3_e.grad.zero_()\n",
    "            w3_d.grad.zero_()\n",
    "            w2_d.grad.zero_()\n",
    "\n",
    "    if t % 10 == 0:\n",
    "        print(f\"Epoch {t}\")\n",
    "\n",
    "# آموزش Perceptron\n",
    "mse_train = torch.zeros(epochs_perceptron, device=device)\n",
    "mse_test = torch.zeros(epochs_perceptron, device=device)\n",
    "\n",
    "for t in range(epochs_perceptron):\n",
    "    for i in range(X_train.shape[0]):\n",
    "        autoencoder_out = compute_encoder_output(X_train[i].unsqueeze(0)).squeeze()  # (10,)\n",
    "        \n",
    "        # Forward pass - Perceptron\n",
    "        net1_p = w1_p @ autoencoder_out  # (5, 10) @ (10,) → (5,)\n",
    "        o1_p = sigmoid(net1_p)           # (5,)\n",
    "        net2_p = w2_p @ o1_p             # (1, 5) @ (5,) → (1,)\n",
    "        o2_p = sigmoid(net2_p)           # (1,)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss = (Y_train[i] - o2_p) ** 2\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            w2_p -= eta_perceptron * w2_p.grad\n",
    "            w1_p -= eta_perceptron * w1_p.grad\n",
    "            w2_p.grad.zero_()\n",
    "            w1_p.grad.zero_()\n",
    "\n",
    "    # پیش‌بینی‌ها\n",
    "    with torch.no_grad():\n",
    "        predictions_train = torch.stack([compute_perceptron_output(compute_encoder_output(x.unsqueeze(0)).squeeze()) for x in X_train]).squeeze()\n",
    "        predictions_test = torch.stack([compute_perceptron_output(compute_encoder_output(x.unsqueeze(0)).squeeze()) for x in X_test]).squeeze()\n",
    "        mse_train[t] = torch.mean((Y_train - predictions_train) ** 2)\n",
    "        mse_test[t] = torch.mean((Y_test - predictions_test) ** 2)\n",
    "\n",
    "    if t % 10 == 0:\n",
    "        print(f\"Epoch {t}\")\n",
    "\n",
    "# انتقال داده‌ها به CPU برای رسم و محاسبات نهایی\n",
    "predictions_train = predictions_train.cpu().numpy()\n",
    "predictions_test = predictions_test.cpu().numpy()\n",
    "Y_train = Y_train.cpu().numpy()\n",
    "Y_test = Y_test.cpu().numpy()\n",
    "mse_train = mse_train.cpu().numpy()\n",
    "mse_test = mse_test.cpu().numpy()\n",
    "\n",
    "# نرمال‌زدایی معکوس\n",
    "predictions_train_denorm = predictions_train * (max_val - min_val).item() + min_val.item()\n",
    "predictions_test_denorm = predictions_test * (max_val - min_val).item() + min_val.item()\n",
    "Y_train_denorm = Y_train * (max_val - min_val).item() + min_val.item()\n",
    "Y_test_denorm = Y_test * (max_val - min_val).item() + min_val.item()\n",
    "\n",
    "# رسم نتایج\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(Y_train_denorm[:100], label=\"Actual\", color=\"blue\", alpha=0.7)\n",
    "plt.plot(predictions_train_denorm[:100], label=\"Predicted\", color=\"red\", alpha=0.7)\n",
    "plt.title(\"Train Data: Actual vs Predicted (Denormalized)\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Temperature (°C)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(Y_test_denorm[:100], label=\"Actual\", color=\"blue\", alpha=0.7)\n",
    "plt.plot(predictions_test_denorm[:100], label=\"Predicted\", color=\"red\", alpha=0.7)\n",
    "plt.title(\"Test Data: Actual vs Predicted (Denormalized)\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Temperature (°C)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(mse_train, label=\"MSE Train\", color=\"blue\")\n",
    "plt.plot(mse_test, label=\"MSE Test\", color=\"red\")\n",
    "plt.title(\"MSE over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(Y_test_denorm, predictions_test_denorm, color=\"purple\", alpha=0.5)\n",
    "plt.plot([Y_test_denorm.min(), Y_test_denorm.max()], [Y_test_denorm.min(), Y_test_denorm.max()], 'k--', lw=2)\n",
    "plt.title(\"Regression: Actual vs Predicted (Test Data)\")\n",
    "plt.xlabel(\"Actual Temperature (°C)\")\n",
    "plt.ylabel(\"Predicted Temperature (°C)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# محاسبه MAE\n",
    "def calculate_mae(actual, predicted):\n",
    "    return np.mean(np.abs(actual - predicted))\n",
    "\n",
    "mae_train = calculate_mae(Y_train, predictions_train)\n",
    "mae_test = calculate_mae(Y_test, predictions_test)\n",
    "mae_train_denorm = calculate_mae(Y_train_denorm, predictions_train_denorm)\n",
    "mae_test_denorm = calculate_mae(Y_test_denorm, predictions_test_denorm)\n",
    "\n",
    "print(\"Performance Metrics (Normalized):\")\n",
    "print(f\"Train MSE: {mse_train[-1]:.6f}\")\n",
    "print(f\"Test MSE: {mse_test[-1]:.6f}\")\n",
    "print(f\"Train MAE: {mae_train:.6f}\")\n",
    "print(f\"Test MAE: {mae_test:.6f}\")\n",
    "\n",
    "print(\"\\nPerformance Metrics (Denormalized):\")\n",
    "print(f\"Train MSE (Denorm): {(mse_train[-1] * (max_val - min_val)**2).item():.6f}\")\n",
    "print(f\"Test MSE (Denorm): {(mse_test[-1] * (max_val - min_val)**2).item():.6f}\")\n",
    "print(f\"Train MAE (Denorm): {mae_train_denorm:.6f}\")\n",
    "print(f\"Test MAE (Denorm): {mae_test_denorm:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7ee59a-9785-4aae-afc8-91ba646161f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
